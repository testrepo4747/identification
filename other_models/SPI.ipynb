{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-rolling",
   "metadata": {},
   "source": [
    "## Import pakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,SimpleRNN,Conv2D,MaxPooling2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and normalize text\n",
    "def clean_text(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "    # Modify this based on the actual preprocessing steps needed from the notebook\n",
    "#     df['manually_label'] = df['manually_label'].map({'positive': 1, 'negative': 0})\n",
    "    \n",
    "    dic_list = []\n",
    "    for index,row in df.iterrows():\n",
    "        res = row['manually_label'] \n",
    "\n",
    "        if res == \"positive\":\n",
    "            row['manually_label'] = 1\n",
    "        elif res == \"negative\":\n",
    "            row['manually_label'] = 0\n",
    "\n",
    "        if res == '1.0':\n",
    "            row['manually_label'] = 1\n",
    "        elif res == '0.0':\n",
    "            row['manually_label'] = 0\n",
    "\n",
    "        if res == '1':\n",
    "            row['manually_label'] = 1\n",
    "        elif res == '0':\n",
    "            row['manually_label'] = 0\n",
    "\n",
    "        dic_list.append(row)\n",
    "    \n",
    "\n",
    "    \n",
    "    df_split = pd.DataFrame(dic_list)\n",
    "#     df_split = a\n",
    "\n",
    "    df_pos = df_split.loc[df_split['manually_label'] == 1]\n",
    "    df_neg = df_split.loc[df_split['manually_label'] == 0]\n",
    "    df_pos1 = df_split.loc[df_split['manually_label'] == '1']\n",
    "    df_neg1 = df_split.loc[df_split['manually_label'] == '0']\n",
    "\n",
    "    print(\"pos:{}, neg:{}\".format(df_pos.shape,df_neg.shape))\n",
    "    print(\"pos:{}, neg:{}\".format(df_pos1.shape,df_neg1.shape))\n",
    "    \n",
    "    df= pd.concat([df_pos,df_pos1, df_neg,df_neg1], ignore_index = True)\n",
    "    df['clean_message'] = df['clean_message'].astype(str)\n",
    "\n",
    "    df['clean_message'] = df['clean_message'].apply(clean_text)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('./train.csv',encoding='utf-8')\n",
    "test_data = pd.read_csv('./test.csv',encoding='utf-8')\n",
    "train_data =  process_dataframe(train_data)\n",
    "test_data =  process_dataframe(test_data)\n",
    "# Settings\n",
    "max_words = 10000 #len(tokenizer.word_index)# 10000  # vocabulary size\n",
    "max_len = 50       # max length of sequences\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_data['clean_message'])\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['clean_message'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['clean_message'])\n",
    "\n",
    "# Pad sequences\n",
    "train_data_padded = pad_sequences(train_sequences, maxlen=max_len)\n",
    "test_data_padded = pad_sequences(test_sequences, maxlen=max_len)\n",
    "\n",
    "# Labels to categorical\n",
    "# Assuming 'manually_label' is a column with binary labels\n",
    "# train_labels = train_data['manually_label'].values\n",
    "# test_labels = test_data['manually_label'].values\n",
    "\n",
    "train_labels = to_categorical(np.asarray(train_data['manually_label']))\n",
    "test_labels = to_categorical(np.asarray(test_data['manually_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin\",binary = True)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "embedding_dim = word2vec.vector_size\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "\n",
    "\n",
    "for word in word_index:\n",
    "    if word in word2vec and word_index[word] < max_words:\n",
    "        embedding_matrix[word_index[word]] = word2vec[word]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-auditor",
   "metadata": {},
   "source": [
    "### SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "#     Embedding(input_dim=max_words, output_dim=300, input_length=max_len),\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length = 50,\n",
    "                   weights = [embedding_matrix],\n",
    "#                    mask_zero=True,\n",
    "                   trainable = False),\n",
    "    Conv1D(filters=64, kernel_size=(3,), strides=(1,),activation='relu'),\n",
    "    MaxPooling1D(pool_size=(2,), strides=2),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(filters=32, kernel_size=(3,), strides=(1,),  activation='relu'),\n",
    "    MaxPooling1D(pool_size=(2,), strides=2),\n",
    "    Dropout(0.2), \n",
    "    LSTM(64),\n",
    "    Dropout(0.2), \n",
    "    Dense(2, activation='softmax')  # Change the number of units based on the number of classes\n",
    "])\n",
    "# Specify the learning rate\n",
    "learning_rate = 0.0001  # or whatever value you want\n",
    "\n",
    "# Configure the optimizer with the desired learning rate\n",
    "adam_optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Output model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-rwanda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.1,patience=50)\n",
    "model.fit(train_data_padded, train_labels, batch_size=64, epochs=200, validation_split=0.2,\n",
    "          callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./data_out/icse/spi_cm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_data_padded, test_labels)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from numpy import argmax\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "# test_predictions = model.predict(test_data_padded).ravel()  # flatten array to 1D if it's not already\n",
    "# test_predictions_classes = (test_predictions > 0.8).astype(int)  # Convert probabilities to class predictions\n",
    "# Predict classes\n",
    "test_predictions = model.predict(test_data_padded)\n",
    "test_predictions_classes = argmax(test_predictions, axis=1)\n",
    "test_true_classes = argmax(test_labels, axis=1)\n",
    "\n",
    "# Test labels should be in a flat array already if you prepared them for binary classification\n",
    "# test_true_classes = test_labels\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_true_classes, test_predictions_classes)\n",
    "precision = precision_score(test_true_classes, test_predictions_classes)\n",
    "recall = recall_score(test_true_classes, test_predictions_classes)\n",
    "f1 = f1_score(test_true_classes, test_predictions_classes)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the entire model back.\n",
    "model = load_model('./data_out/icse/patch_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_data_padded, test_labels)\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from numpy import argmax\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "# test_predictions = model.predict(test_data_padded).ravel()  # flatten array to 1D if it's not already\n",
    "# test_predictions_classes = (test_predictions > 0.8).astype(int)  # Convert probabilities to class predictions\n",
    "# Predict classes\n",
    "test_predictions = model.predict(test_data_padded)\n",
    "test_predictions_classes = argmax(test_predictions, axis=1)\n",
    "test_true_classes = argmax(test_labels, axis=1)\n",
    "\n",
    "# Test labels should be in a flat array already if you prepared them for binary classification\n",
    "# test_true_classes = test_labels\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_true_classes, test_predictions_classes)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_true_classes, test_predictions_classes)\n",
    "precision = precision_score(test_true_classes, test_predictions_classes)\n",
    "recall = recall_score(test_true_classes, test_predictions_classes)\n",
    "f1 = f1_score(test_true_classes, test_predictions_classes)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "comfmat = pd.DataFrame(confusion_matrix(test_true_classes, test_predictions_classes), index=['negative', 'positive'],columns=['negative', 'positive'])\n",
    "comfmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# android_tf_data = pd.read_csv('/working/BERT/data/android_x/android_tf_opencv.csv')\n",
    "\n",
    "# print(android_tf_data['manually_label'].value_counts())\n",
    "# android_tf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-lingerie",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
